{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Setup","metadata":{}},{"cell_type":"code","source":"!pip -q install transformers\n!pip -q install evaluate\n!pip -q install rouge_score","metadata":{"execution":{"iopub.status.busy":"2023-10-01T13:52:43.028207Z","iopub.execute_input":"2023-10-01T13:52:43.029046Z","iopub.status.idle":"2023-10-01T13:53:16.264269Z","shell.execute_reply.started":"2023-10-01T13:52:43.029015Z","shell.execute_reply":"2023-10-01T13:53:16.262948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from glob import glob\nfrom sklearn.model_selection import GroupKFold, StratifiedKFold\nimport torch\nfrom torch import nn\nimport os\nimport itertools\nfrom datetime import datetime\nimport time\nimport random\nimport pprint\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.utils.data.sampler import SequentialSampler, RandomSampler\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torch.nn.modules.loss import _WeightedLoss\nimport torch.nn.functional as F\n\nimport sklearn\nimport warnings\nimport joblib\nfrom sklearn.metrics import roc_auc_score, log_loss\nfrom sklearn import metrics\n\n# Hugging Face\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport evaluate\n\n# rich: for a better display on terminal\nfrom rich.table import Column, Table\nfrom rich import box\nfrom rich.console import Console\n\n# Device\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f'Running on device: {device}')","metadata":{"execution":{"iopub.status.busy":"2023-10-01T13:53:16.266498Z","iopub.execute_input":"2023-10-01T13:53:16.267157Z","iopub.status.idle":"2023-10-01T13:53:32.195552Z","shell.execute_reply.started":"2023-10-01T13:53:16.267120Z","shell.execute_reply":"2023-10-01T13:53:32.194591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CFG = {\n    'show_examples': False,\n    'data_dir': '/kaggle/input/drowsy-eye-keypoints',\n    'cluster_sent_n': 20,\n    'seed': 719,\n    'model_arch': \"VietAI/vit5-base-vietnews-summarization\",\n    'source_len': 1024,\n    'target_len': 256,\n    'epochs': 3,\n    'train_bs': 2,\n    'valid_bs': 2,\n    'T_0': 10,\n    'lr': 1e-4,\n    'min_lr': 1e-6,\n    'weight_decay':1e-6,\n    'num_workers': 4,\n    'accum_iter': 2, # suppoprt to do batch accumulation for backprop with effectively larger batch size\n    'verbose_step': 1,\n}","metadata":{"execution":{"iopub.status.busy":"2023-10-01T13:53:32.197003Z","iopub.execute_input":"2023-10-01T13:53:32.197875Z","iopub.status.idle":"2023-10-01T13:53:32.204218Z","shell.execute_reply.started":"2023-10-01T13:53:32.197842Z","shell.execute_reply":"2023-10-01T13:53:32.203256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir = '/kaggle/input/vims-dataset/ViMs'\noriginal_dir = os.path.join(data_dir, 'original')\nsummary_dir = os.path.join(data_dir, 'summary')","metadata":{"execution":{"iopub.status.busy":"2023-10-01T13:53:32.207131Z","iopub.execute_input":"2023-10-01T13:53:32.208515Z","iopub.status.idle":"2023-10-01T13:53:32.224573Z","shell.execute_reply.started":"2023-10-01T13:53:32.208483Z","shell.execute_reply":"2023-10-01T13:53:32.223547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Utils","metadata":{}},{"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True","metadata":{"execution":{"iopub.status.busy":"2023-10-01T13:53:32.226152Z","iopub.execute_input":"2023-10-01T13:53:32.226896Z","iopub.status.idle":"2023-10-01T13:53:32.238109Z","shell.execute_reply.started":"2023-10-01T13:53:32.226864Z","shell.execute_reply":"2023-10-01T13:53:32.237061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define a rich console logger\nconsole = Console(record=True)\n\n# to display dataframe in ASCII format\ndef display_df(df):\n    \"\"\"display dataframe in ASCII format\"\"\"\n\n    console = Console()\n    table = Table(\n        Column(\"source_text\", justify=\"center\"),\n        Column(\"target_text\", justify=\"center\"),\n        title=\"Sample Data\",\n        pad_edge=False,\n        box=box.ASCII,\n    )\n\n    for i, row in enumerate(df.values.tolist()):\n        table.add_row(row[0], row[1])\n\n    console.print(table)\n\n# training logger to log training progress\ntraining_logger = Table()\n\ndef resetTable():\n    global training_logger\n\n    training_logger = Table(\n    Column(\"Epoch\", justify=\"center\"),\n    Column(\"Steps\", justify=\"center\"),\n    Column(\"Loss\", justify=\"center\"),\n    title=\"Training Status\",\n    pad_edge=False,\n    box=box.ASCII,\n)\nresetTable()","metadata":{"execution":{"iopub.status.busy":"2023-10-01T13:53:32.239763Z","iopub.execute_input":"2023-10-01T13:53:32.240634Z","iopub.status.idle":"2023-10-01T13:53:32.253507Z","shell.execute_reply.started":"2023-10-01T13:53:32.240607Z","shell.execute_reply":"2023-10-01T13:53:32.252628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_txt(path, article_type, sent=False):\n    content = []\n    write_file = False\n    with open(path) as f:\n        for line in f:\n            if article_type == \"original\":\n                if line.lower().startswith(\"content\"):\n                    write_file = True\n            else:\n                write_file = True\n            if write_file: \n                if line.rstrip():\n                    content.append(line.rstrip())\n    if sent:\n        return content[1:]\n    return \" \".join(content[1:])\nif CFG['show_examples']:\n    path1 = '/kaggle/input/vims-dataset/ViMs/original/Cluster_001/original/10.txt'\n    path2 = '/kaggle/input/vims-dataset/ViMs/summary/Cluster_001/0.gold.txt'\n    print(read_txt(path2, article_type=\"summary\", sent=True))","metadata":{"execution":{"iopub.status.busy":"2023-10-01T13:53:32.254539Z","iopub.execute_input":"2023-10-01T13:53:32.254872Z","iopub.status.idle":"2023-10-01T13:53:32.266461Z","shell.execute_reply.started":"2023-10-01T13:53:32.254841Z","shell.execute_reply":"2023-10-01T13:53:32.265524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CSV File","metadata":{}},{"cell_type":"code","source":"def create_csv(data_dir):\n    \"\"\"\n    Input: data_dir\n    - dir format: data_dir/original/cluster/original/txt\n    Output: csv\n    \"\"\"\n    df = {'cluster':[], 'path':[]}\n    for cluster in os.listdir(data_dir):\n        file_type = data_dir[data_dir.rfind(\"/\")+1:]\n        if file_type == \"original\":\n            f_path = os.path.join(data_dir, cluster, file_type)\n        else:\n            f_path = os.path.join(data_dir, cluster)\n        for f in glob(f_path + '/*'):\n            df['cluster'].append(cluster)\n            df['path'].append(f)\n\n    df = pd.DataFrame(df)\n    df = df.groupby('cluster')['path'].apply(list).reset_index()\n    return df\n\noriginal_df = create_csv(original_dir)\noriginal_df.columns = ['cluster', 'original_dir']\n\nsummary_df = create_csv(summary_dir)\nsummary_df.columns = ['cluster', 'summary_dir']\n\ndf = original_df.merge(summary_df, how='inner', on='cluster')\nif CFG['show_examples']:\n    print(len(df))\n    print(len(df['cluster'].unique()))\n    print(df.head())","metadata":{"execution":{"iopub.status.busy":"2023-10-01T13:53:32.267851Z","iopub.execute_input":"2023-10-01T13:53:32.268518Z","iopub.status.idle":"2023-10-01T13:53:35.482537Z","shell.execute_reply.started":"2023-10-01T13:53:32.268489Z","shell.execute_reply":"2023-10-01T13:53:35.481338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset","metadata":{}},{"cell_type":"code","source":"class MyDataset(Dataset):\n\n    def __init__(\n        self, dataframe, tokenizer, source_len, target_len, source_dir=\"original_dir\", target_dir=\"summary_dir\"\n    ):\n        \"\"\"\n        Initializes a Dataset class\n\n        Args:\n            dataframe (pandas.DataFrame): Input dataframe\n            tokenizer (transformers.tokenizer): Transformers tokenizer\n            source_len (int): Max length of source text\n            target_len (int): Max length of target text\n        \"\"\"\n        self.tokenizer = tokenizer\n        self.df = dataframe\n        self.source_len = source_len\n        self.target_len = target_len\n        self.source_dir = self.df[source_dir]\n        self.target_dir = self.df[target_dir]\n\n    def __len__(self):\n        \"\"\"returns the length of dataframe\"\"\"\n\n        return len(self.df)\n\n    def __getitem__(self, index):\n        \"\"\"return the input ids, attention masks and target ids\"\"\"\n        \n        ## Source text\n        source_dir_list = self.source_dir[index]\n        source_text = []\n        for i in range(len(source_dir_list)):\n            txt = read_txt(source_dir_list[i], article_type=\"original\", sent=True)\n            source_text += txt\n        \n        source_ids = random.sample(range(len(source_text)), \n                                   min(CFG['cluster_sent_n'], len(source_text)))\n        source_ids = sorted(source_ids)\n        source_text = \" \".join([source_text[i] for i in source_ids])\n        \n        ## Target text\n        target_id = random.choice([0,1])\n        target_dir_list = self.target_dir[index]\n        target_text = read_txt(target_dir_list[target_id], article_type=\"summary\")\n        \n        ## Tokenize\n        source = self.tokenizer.batch_encode_plus(\n            [source_text],\n            max_length=self.source_len,\n            pad_to_max_length=True,\n            truncation=True,\n            padding=\"max_length\",\n            return_tensors=\"pt\",\n        )\n\n        target = self.tokenizer.batch_encode_plus(\n            [target_text],\n            max_length=self.target_len,\n            pad_to_max_length=True,\n            truncation=True,\n            padding=\"max_length\",\n            return_tensors=\"pt\",\n        )\n\n        source_ids = source[\"input_ids\"].squeeze()\n        source_mask = source[\"attention_mask\"].squeeze()\n        target_ids = target[\"input_ids\"].squeeze()\n        target_mask = target[\"attention_mask\"].squeeze()\n\n        return {\n            \"source_txt\": source_text,\n            \"target_txt\": target_text,\n            \"source_ids\": source_ids.to(dtype=torch.long),\n            \"source_mask\": source_mask.to(dtype=torch.long),\n            \"target_ids\": target_ids.to(dtype=torch.long),\n            \"target_mask\": target_mask.to(dtype=torch.long),\n        }\n\nif CFG['show_examples']:\n    tokenizer = AutoTokenizer.from_pretrained(CFG['model_arch'])\n    dataset = MyDataset(df, tokenizer, CFG['source_len'], CFG['target_len'])\n    for i, output in enumerate(dataset):\n        print(\"Source:\")\n        print(\"Source article: \\n\", output['source_txt'])\n        print(\"Source input ids length: \\n\", len(output['source_ids']))\n        print(\"Source input ids: \\n\", output['source_ids'])\n        print(\"Source attention mask: \\n\", output['source_mask'])\n        print(\"\\n\")\n        print(\"Target:\\n\")\n        print(\"Target text: \\n\", output['source_txt'])\n        print(\"Target input ids length: \\n\", len(output['source_ids']))\n        print(\"Target attention mask: \\n\", output['source_mask'])\n        if i >= 0:\n            break","metadata":{"execution":{"iopub.status.busy":"2023-10-01T13:53:35.484354Z","iopub.execute_input":"2023-10-01T13:53:35.485095Z","iopub.status.idle":"2023-10-01T13:53:35.501574Z","shell.execute_reply.started":"2023-10-01T13:53:35.485057Z","shell.execute_reply":"2023-10-01T13:53:35.500091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train","metadata":{}},{"cell_type":"code","source":"def train(epoch, tokenizer, model, device, loader, optimizer):\n\n    \"\"\"\n    Function to be called for training with the parameters passed from main function\n\n    \"\"\"\n\n    model.train()\n    for _, data in enumerate(loader, 0):\n        y = data[\"target_ids\"].to(device, dtype=torch.long)\n        y_ids = y[:, :-1].contiguous()\n        lm_labels = y[:, 1:].clone().detach()\n        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n        ids = data[\"source_ids\"].to(device, dtype=torch.long)\n        mask = data[\"source_mask\"].to(device, dtype=torch.long)\n\n        outputs = model(\n            input_ids=ids,\n            attention_mask=mask,\n            decoder_input_ids=y_ids,\n            labels=lm_labels,\n        )\n        loss = outputs[0]\n\n        if _ % 20 == 0:\n            training_logger.add_row(str(epoch), str(_), str(loss))\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    \n    console.print(training_logger)\n    resetTable()","metadata":{"execution":{"iopub.status.busy":"2023-10-01T13:53:35.505662Z","iopub.execute_input":"2023-10-01T13:53:35.505988Z","iopub.status.idle":"2023-10-01T13:53:35.521437Z","shell.execute_reply.started":"2023-10-01T13:53:35.505964Z","shell.execute_reply":"2023-10-01T13:53:35.520524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def validate(epoch, tokenizer, model, device, loader):\n    \"\"\"\n    Function to evaluate model for predictions\n    \"\"\"\n    model.eval()\n    \n    predictions = []\n    actuals = []\n    with torch.no_grad():\n        for _, data in enumerate(loader, 0):\n            y = data['target_ids'].to(device, dtype = torch.long)\n            ids = data['source_ids'].to(device, dtype = torch.long)\n            mask = data['source_mask'].to(device, dtype = torch.long)\n        \n            generated_ids = model.generate(\n                input_ids = ids,\n                attention_mask = mask, \n                max_length=256,\n                num_beams=5,\n                repetition_penalty=2.5,\n                length_penalty=1.0,\n                early_stopping=True\n            )\n            \n            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n            \n            if _%20==0:\n                console.print(f'Completed {_}')\n            \n            predictions.extend(preds)\n            actuals.extend(target)\n\n    # print ROUGE score\n    rouge = evaluate.load('rouge')\n    results = rouge.compute(predictions=predictions,\n                            references=actuals)\n    console.print(\"ROUGE: \", results)\n    \n    return predictions, actuals","metadata":{"execution":{"iopub.status.busy":"2023-10-01T13:53:35.522741Z","iopub.execute_input":"2023-10-01T13:53:35.523372Z","iopub.status.idle":"2023-10-01T13:53:35.537113Z","shell.execute_reply.started":"2023-10-01T13:53:35.523338Z","shell.execute_reply":"2023-10-01T13:53:35.536085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def T5Trainer(dataframe, output_dir=\"/kaggle/working/\"):\n\n    \"\"\"\n    T5 trainer\n    \"\"\"\n    # Set random seeds and deterministic pytorch for reproducibility\n    seed_everything(CFG['seed'])\n\n    # logging\n    console.log(f\"\"\"[Model]: Loading {CFG[\"model_arch\"]}...\\n\"\"\")\n\n    # tokenzier for encoding the text\n    tokenizer = AutoTokenizer.from_pretrained(CFG['model_arch'])\n\n    # Defining the model\n    model = AutoModelForSeq2SeqLM.from_pretrained(CFG['model_arch'])\n    model = model.to(device)\n\n    # logging\n    console.log(f\"[Data]: Reading data...\\n\")\n\n    # Creation of Dataset and Dataloader\n    global val_dataset\n    \n    train_size = 0.8\n    train_dataset = dataframe.sample(frac=train_size, random_state=CFG[\"seed\"])\n    val_dataset = dataframe.drop(train_dataset.index).reset_index(drop=True)\n    train_dataset = train_dataset.reset_index(drop=True)\n\n    console.print(f\"FULL Dataset: {dataframe.shape}\")\n    console.print(f\"TRAIN Dataset: {train_dataset.shape}\")\n    console.print(f\"TEST Dataset: {val_dataset.shape}\\n\")\n\n    # Creating the Training and Validation dataset for further creation of Dataloader\n    training_set = MyDataset(\n        train_dataset,\n        tokenizer,\n        CFG['source_len'], \n        CFG['target_len']\n    )\n    val_set = MyDataset(\n        val_dataset,\n        tokenizer,\n        CFG['source_len'], \n        CFG['target_len']\n    )\n\n    # Defining the parameters for creation of dataloaders\n    train_params = {\n        \"batch_size\": CFG[\"train_bs\"],\n        \"shuffle\": True,\n        \"num_workers\": 0,\n    }\n\n    val_params = {\n        \"batch_size\": CFG[\"valid_bs\"],\n        \"shuffle\": False,\n        \"num_workers\": 0,\n    }\n\n    # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n    training_loader = DataLoader(training_set, **train_params)\n    val_loader = DataLoader(val_set, **val_params)\n\n    # Defining the optimizer that will be used to tune the weights of the network in the training session.\n    optimizer = torch.optim.Adam(\n        params=model.parameters(), lr=CFG[\"lr\"]\n    )\n\n    # Training loop\n    console.log(f\"[Initiating Fine Tuning]...\\n\")\n\n    for epoch in range(CFG[\"epochs\"]):\n        train(epoch, tokenizer, model, device, training_loader, optimizer)\n\n    console.log(f\"[Saving Model]...\\n\")\n    # Saving the model after training\n    path = os.path.join(output_dir, \"model_files\")\n    model.save_pretrained(path)\n    tokenizer.save_pretrained(path)\n\n    # evaluating test dataset\n    global final_df\n    console.log(f\"[Initiating Validation]...\\n\")\n    predictions, actuals = validate(0, tokenizer, model, device, val_loader)\n    final_df = pd.DataFrame({\"Generated Text\": predictions, \"Actual Text\": actuals})\n    final_df.to_csv(os.path.join(output_dir, \"predictions.csv\"))\n\n    console.save_text(os.path.join(output_dir, \"logs.txt\"))\n\n    console.log(f\"[Validation Completed.]\\n\")\n    console.print(\n        f\"\"\"[Model] Model saved @ {os.path.join(output_dir, \"model_files\")}\\n\"\"\"\n    )\n    console.print(\n        f\"\"\"[Validation] Generation on Validation data saved @ {os.path.join(output_dir,'predictions.csv')}\\n\"\"\"\n    )\n    console.print(f\"\"\"[Logs] Logs saved @ {os.path.join(output_dir,'logs.txt')}\\n\"\"\")","metadata":{"execution":{"iopub.status.busy":"2023-10-01T13:53:35.538957Z","iopub.execute_input":"2023-10-01T13:53:35.539710Z","iopub.status.idle":"2023-10-01T13:53:35.554909Z","shell.execute_reply.started":"2023-10-01T13:53:35.539676Z","shell.execute_reply":"2023-10-01T13:53:35.553808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Main","metadata":{}},{"cell_type":"code","source":"if __name__ == '__main__':\n    T5Trainer(df)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T13:53:35.556645Z","iopub.execute_input":"2023-10-01T13:53:35.557333Z","iopub.status.idle":"2023-10-01T13:58:30.903470Z","shell.execute_reply.started":"2023-10-01T13:53:35.557299Z","shell.execute_reply":"2023-10-01T13:58:30.902357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Print result examples","metadata":{}},{"cell_type":"code","source":"# Pick randomly a cluster index\nrd_idx = random.randint(0,len(val_dataset))","metadata":{"execution":{"iopub.status.busy":"2023-10-01T14:03:45.017751Z","iopub.execute_input":"2023-10-01T14:03:45.018107Z","iopub.status.idle":"2023-10-01T14:03:45.023468Z","shell.execute_reply.started":"2023-10-01T14:03:45.018081Z","shell.execute_reply":"2023-10-01T14:03:45.022008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Source text\nprint(\"Source: \\n\")\nsource_dir_list = val_dataset[\"original_dir\"][rd_idx]\nsource_text = []\nfor i in range(len(source_dir_list)):\n    txt = read_txt(source_dir_list[i], article_type=\"original\")\n    print(txt)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T14:03:47.765336Z","iopub.execute_input":"2023-10-01T14:03:47.765747Z","iopub.status.idle":"2023-10-01T14:03:47.780753Z","shell.execute_reply.started":"2023-10-01T14:03:47.765718Z","shell.execute_reply":"2023-10-01T14:03:47.779661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Actual\nprint(\"Actual: \\n\")\nfinal_df['Actual Text'][rd_idx]","metadata":{"execution":{"iopub.status.busy":"2023-10-01T14:03:53.005435Z","iopub.execute_input":"2023-10-01T14:03:53.005792Z","iopub.status.idle":"2023-10-01T14:03:53.013132Z","shell.execute_reply.started":"2023-10-01T14:03:53.005767Z","shell.execute_reply":"2023-10-01T14:03:53.011979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Prediction\nprint(\"Prediction: \\n\")\nfinal_df['Generated Text'][rd_idx]","metadata":{"execution":{"iopub.status.busy":"2023-10-01T14:03:57.469375Z","iopub.execute_input":"2023-10-01T14:03:57.469789Z","iopub.status.idle":"2023-10-01T14:03:57.477913Z","shell.execute_reply.started":"2023-10-01T14:03:57.469759Z","shell.execute_reply":"2023-10-01T14:03:57.476909Z"},"trusted":true},"execution_count":null,"outputs":[]}]}