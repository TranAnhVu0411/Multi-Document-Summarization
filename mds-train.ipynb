{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Setup","metadata":{}},{"cell_type":"code","source":"!pip -q install transformers","metadata":{"execution":{"iopub.status.busy":"2023-10-01T09:30:01.918139Z","iopub.execute_input":"2023-10-01T09:30:01.918823Z","iopub.status.idle":"2023-10-01T09:30:10.785531Z","shell.execute_reply.started":"2023-10-01T09:30:01.918792Z","shell.execute_reply":"2023-10-01T09:30:10.784353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from glob import glob\nfrom sklearn.model_selection import GroupKFold, StratifiedKFold\nimport torch\nfrom torch import nn\nimport os\nimport itertools\nfrom datetime import datetime\nimport time\nimport random\nimport torchvision\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.utils.data.sampler import SequentialSampler, RandomSampler\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torch.nn.modules.loss import _WeightedLoss\nimport torch.nn.functional as F\n\nimport sklearn\nimport warnings\nimport joblib\nfrom sklearn.metrics import roc_auc_score, log_loss\nfrom sklearn import metrics\n\n# Hugging Face\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\n# rich: for a better display on terminal\nfrom rich.table import Column, Table\nfrom rich import box\nfrom rich.console import Console\n\n# Device\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f'Running on device: {device}')","metadata":{"execution":{"iopub.status.busy":"2023-10-01T09:30:10.787876Z","iopub.execute_input":"2023-10-01T09:30:10.788173Z","iopub.status.idle":"2023-10-01T09:30:10.797557Z","shell.execute_reply.started":"2023-10-01T09:30:10.788147Z","shell.execute_reply":"2023-10-01T09:30:10.796621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CFG = {\n    'show_examples': False,\n    'data_dir': '/kaggle/input/drowsy-eye-keypoints',\n    'cluster_sent_n': 20,\n    'seed': 719,\n    'model_arch': \"VietAI/vit5-base-vietnews-summarization\",\n    'source_len': 1024,\n    'target_len': 256,\n    'epochs': 3,\n    'train_bs': 2,\n    'valid_bs': 2,\n    'T_0': 10,\n    'lr': 1e-4,\n    'min_lr': 1e-6,\n    'weight_decay':1e-6,\n    'num_workers': 4,\n    'accum_iter': 2, # suppoprt to do batch accumulation for backprop with effectively larger batch size\n    'verbose_step': 1,\n}","metadata":{"execution":{"iopub.status.busy":"2023-10-01T09:30:10.798759Z","iopub.execute_input":"2023-10-01T09:30:10.799056Z","iopub.status.idle":"2023-10-01T09:30:10.817538Z","shell.execute_reply.started":"2023-10-01T09:30:10.799025Z","shell.execute_reply":"2023-10-01T09:30:10.816548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir = '/kaggle/input/vims-dataset/ViMs'\noriginal_dir = os.path.join(data_dir, 'original')\nsummary_dir = os.path.join(data_dir, 'summary')","metadata":{"execution":{"iopub.status.busy":"2023-10-01T09:30:10.819954Z","iopub.execute_input":"2023-10-01T09:30:10.820536Z","iopub.status.idle":"2023-10-01T09:30:10.831328Z","shell.execute_reply.started":"2023-10-01T09:30:10.820500Z","shell.execute_reply":"2023-10-01T09:30:10.830399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Utils","metadata":{}},{"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True","metadata":{"execution":{"iopub.status.busy":"2023-10-01T09:30:10.832709Z","iopub.execute_input":"2023-10-01T09:30:10.833210Z","iopub.status.idle":"2023-10-01T09:30:10.844721Z","shell.execute_reply.started":"2023-10-01T09:30:10.833180Z","shell.execute_reply":"2023-10-01T09:30:10.843770Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define a rich console logger\nconsole = Console(record=True)\n\n# to display dataframe in ASCII format\ndef display_df(df):\n    \"\"\"display dataframe in ASCII format\"\"\"\n\n    console = Console()\n    table = Table(\n        Column(\"source_text\", justify=\"center\"),\n        Column(\"target_text\", justify=\"center\"),\n        title=\"Sample Data\",\n        pad_edge=False,\n        box=box.ASCII,\n    )\n\n    for i, row in enumerate(df.values.tolist()):\n        table.add_row(row[0], row[1])\n\n    console.print(table)\n\n# training logger to log training progress\ntraining_logger = Table()\n\ndef resetTable():\n    global training_logger\n\n    training_logger = Table(\n    Column(\"Epoch\", justify=\"center\"),\n    Column(\"Steps\", justify=\"center\"),\n    Column(\"Loss\", justify=\"center\"),\n    title=\"Training Status\",\n    pad_edge=False,\n    box=box.ASCII,\n)\nresetTable()","metadata":{"execution":{"iopub.status.busy":"2023-10-01T09:30:10.846135Z","iopub.execute_input":"2023-10-01T09:30:10.846708Z","iopub.status.idle":"2023-10-01T09:30:10.857427Z","shell.execute_reply.started":"2023-10-01T09:30:10.846678Z","shell.execute_reply":"2023-10-01T09:30:10.856397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_txt(path, article_type, sent=False):\n    content = []\n    write_file = False\n    with open(path) as f:\n        for line in f:\n            if article_type == \"original\":\n                if line.lower().startswith(\"content\"):\n                    write_file = True\n            else:\n                write_file = True\n            if write_file: \n                if line.rstrip():\n                    content.append(line.rstrip())\n    if sent:\n        return content[1:]\n    return \" \".join(content[1:])\nif CFG['show_examples']:\n    path1 = '/kaggle/input/vims-dataset/ViMs/original/Cluster_001/original/10.txt'\n    path2 = '/kaggle/input/vims-dataset/ViMs/summary/Cluster_001/0.gold.txt'\n    print(read_txt(path2, article_type=\"summary\", sent=True))","metadata":{"execution":{"iopub.status.busy":"2023-10-01T09:30:10.858863Z","iopub.execute_input":"2023-10-01T09:30:10.859423Z","iopub.status.idle":"2023-10-01T09:30:10.874042Z","shell.execute_reply.started":"2023-10-01T09:30:10.859393Z","shell.execute_reply":"2023-10-01T09:30:10.873098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CSV File","metadata":{}},{"cell_type":"code","source":"def create_csv(data_dir):\n    \"\"\"\n    Input: data_dir\n    - dir format: data_dir/original/cluster/original/txt\n    Output: csv\n    \"\"\"\n    df = {'cluster':[], 'path':[]}\n    for cluster in os.listdir(data_dir):\n        file_type = data_dir[data_dir.rfind(\"/\")+1:]\n        if file_type == \"original\":\n            f_path = os.path.join(data_dir, cluster, file_type)\n        else:\n            f_path = os.path.join(data_dir, cluster)\n        for f in glob(f_path + '/*'):\n            df['cluster'].append(cluster)\n            df['path'].append(f)\n\n    df = pd.DataFrame(df)\n    df = df.groupby('cluster')['path'].apply(list).reset_index()\n    return df\n\noriginal_df = create_csv(original_dir)\noriginal_df.columns = ['cluster', 'original_dir']\n\nsummary_df = create_csv(summary_dir)\nsummary_df.columns = ['cluster', 'summary_dir']\n\ndf = original_df.merge(summary_df, how='inner', on='cluster')\nif CFG['show_examples']:\n    print(len(df))\n    print(len(df['cluster'].unique()))\n    print(df.head())","metadata":{"execution":{"iopub.status.busy":"2023-10-01T09:30:10.875681Z","iopub.execute_input":"2023-10-01T09:30:10.876345Z","iopub.status.idle":"2023-10-01T09:30:11.359949Z","shell.execute_reply.started":"2023-10-01T09:30:10.876313Z","shell.execute_reply":"2023-10-01T09:30:11.358813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset","metadata":{}},{"cell_type":"code","source":"class MyDataset(Dataset):\n\n    def __init__(\n        self, dataframe, tokenizer, source_len, target_len, source_dir=\"original_dir\", target_dir=\"summary_dir\"\n    ):\n        \"\"\"\n        Initializes a Dataset class\n\n        Args:\n            dataframe (pandas.DataFrame): Input dataframe\n            tokenizer (transformers.tokenizer): Transformers tokenizer\n            source_len (int): Max length of source text\n            target_len (int): Max length of target text\n        \"\"\"\n        self.tokenizer = tokenizer\n        self.df = dataframe\n        self.source_len = source_len\n        self.target_len = target_len\n        self.source_dir = self.df[source_dir]\n        self.target_dir = self.df[target_dir]\n\n    def __len__(self):\n        \"\"\"returns the length of dataframe\"\"\"\n\n        return len(self.df)\n\n    def __getitem__(self, index):\n        \"\"\"return the input ids, attention masks and target ids\"\"\"\n        \n        ## Source text\n        source_dir_list = self.source_dir[index]\n        source_text = []\n        for i in range(len(source_dir_list)):\n            txt = read_txt(source_dir_list[i], article_type=\"original\", sent=True)\n            source_text += txt\n        \n        source_ids = random.sample(range(len(source_text)), \n                                   min(CFG['cluster_sent_n'], len(source_text)))\n        source_ids = sorted(source_ids)\n        source_text = \" \".join([source_text[i] for i in source_ids])\n        \n        ## Target text\n        target_id = random.choice([0,1])\n        target_dir_list = self.target_dir[index]\n        target_text = read_txt(target_dir_list[target_id], article_type=\"summary\")\n        \n        ## Tokenize\n        source = self.tokenizer.batch_encode_plus(\n            [source_text],\n            max_length=self.source_len,\n            pad_to_max_length=True,\n            truncation=True,\n            padding=\"max_length\",\n            return_tensors=\"pt\",\n        )\n\n        target = self.tokenizer.batch_encode_plus(\n            [target_text],\n            max_length=self.target_len,\n            pad_to_max_length=True,\n            truncation=True,\n            padding=\"max_length\",\n            return_tensors=\"pt\",\n        )\n\n        source_ids = source[\"input_ids\"].squeeze()\n        source_mask = source[\"attention_mask\"].squeeze()\n        target_ids = target[\"input_ids\"].squeeze()\n        target_mask = target[\"attention_mask\"].squeeze()\n\n        return {\n            \"source_txt\": source_text,\n            \"target_txt\": target_text,\n            \"source_ids\": source_ids.to(dtype=torch.long),\n            \"source_mask\": source_mask.to(dtype=torch.long),\n            \"target_ids\": target_ids.to(dtype=torch.long),\n            \"target_mask\": target_mask.to(dtype=torch.long),\n        }\n\nif CFG['show_examples']:\n    tokenizer = AutoTokenizer.from_pretrained(CFG['model_arch'])\n    dataset = MyDataset(df, tokenizer, CFG['source_len'], CFG['target_len'])\n    for i, output in enumerate(dataset):\n        print(\"Source:\")\n        print(\"Source article: \\n\", output['source_txt'])\n        print(\"Source input ids length: \\n\", len(output['source_ids']))\n        print(\"Source input ids: \\n\", output['source_ids'])\n        print(\"Source attention mask: \\n\", output['source_mask'])\n        print(\"\\n\")\n        print(\"Target:\\n\")\n        print(\"Target text: \\n\", output['source_txt'])\n        print(\"Target input ids length: \\n\", len(output['source_ids']))\n        print(\"Target attention mask: \\n\", output['source_mask'])\n        if i >= 0:\n            break","metadata":{"execution":{"iopub.status.busy":"2023-10-01T09:30:11.361721Z","iopub.execute_input":"2023-10-01T09:30:11.362099Z","iopub.status.idle":"2023-10-01T09:30:11.375786Z","shell.execute_reply.started":"2023-10-01T09:30:11.362064Z","shell.execute_reply":"2023-10-01T09:30:11.374824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train","metadata":{}},{"cell_type":"code","source":"def train(epoch, tokenizer, model, device, loader, optimizer):\n\n    \"\"\"\n    Function to be called for training with the parameters passed from main function\n\n    \"\"\"\n\n    model.train()\n    for _, data in enumerate(loader, 0):\n        y = data[\"target_ids\"].to(device, dtype=torch.long)\n        y_ids = y[:, :-1].contiguous()\n        lm_labels = y[:, 1:].clone().detach()\n        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n        ids = data[\"source_ids\"].to(device, dtype=torch.long)\n        mask = data[\"source_mask\"].to(device, dtype=torch.long)\n\n        outputs = model(\n            input_ids=ids,\n            attention_mask=mask,\n            decoder_input_ids=y_ids,\n            labels=lm_labels,\n        )\n        loss = outputs[0]\n\n        if _ % 10 == 0:\n            training_logger.add_row(str(epoch), str(_), str(loss))\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    \n    console.print(training_logger)\n    resetTable()","metadata":{"execution":{"iopub.status.busy":"2023-10-01T09:30:11.379657Z","iopub.execute_input":"2023-10-01T09:30:11.379975Z","iopub.status.idle":"2023-10-01T09:30:11.394081Z","shell.execute_reply.started":"2023-10-01T09:30:11.379952Z","shell.execute_reply":"2023-10-01T09:30:11.393002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def validate(epoch, tokenizer, model, device, loader):\n\n  \"\"\"\n  Function to evaluate model for predictions\n\n  \"\"\"\n  model.eval()\n  predictions = []\n  actuals = []\n  with torch.no_grad():\n      for _, data in enumerate(loader, 0):\n          y = data['target_ids'].to(device, dtype = torch.long)\n          ids = data['source_ids'].to(device, dtype = torch.long)\n          mask = data['source_mask'].to(device, dtype = torch.long)\n\n          generated_ids = model.generate(\n              input_ids = ids,\n              attention_mask = mask, \n              max_length=256, \n              num_beams=2,\n              repetition_penalty=2.5, \n              length_penalty=1.0, \n              early_stopping=True\n              )\n          preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n          target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n          if _%10==0:\n              console.print(f'Completed {_}')\n\n          predictions.extend(preds)\n          actuals.extend(target)\n  return predictions, actuals","metadata":{"execution":{"iopub.status.busy":"2023-10-01T09:30:11.395367Z","iopub.execute_input":"2023-10-01T09:30:11.395892Z","iopub.status.idle":"2023-10-01T09:30:11.410821Z","shell.execute_reply.started":"2023-10-01T09:30:11.395862Z","shell.execute_reply":"2023-10-01T09:30:11.409742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def T5Trainer(dataframe, output_dir=\"/kaggle/working/\"):\n\n    \"\"\"\n    T5 trainer\n    \"\"\"\n\n    # Set random seeds and deterministic pytorch for reproducibility\n    seed_everything(CFG['seed'])\n\n    # logging\n    console.log(f\"\"\"[Model]: Loading {CFG[\"model_arch\"]}...\\n\"\"\")\n\n    # tokenzier for encoding the text\n    tokenizer = AutoTokenizer.from_pretrained(CFG['model_arch'])\n\n    # Defining the model\n    model = AutoModelForSeq2SeqLM.from_pretrained(CFG['model_arch'])\n    model = model.to(device)\n\n    # logging\n    console.log(f\"[Data]: Reading data...\\n\")\n\n    # Creation of Dataset and Dataloader\n    train_size = 0.8\n    train_dataset = dataframe.sample(frac=train_size, random_state=CFG[\"seed\"])\n    val_dataset = dataframe.drop(train_dataset.index).reset_index(drop=True)\n    train_dataset = train_dataset.reset_index(drop=True)\n\n    console.print(f\"FULL Dataset: {dataframe.shape}\")\n    console.print(f\"TRAIN Dataset: {train_dataset.shape}\")\n    console.print(f\"TEST Dataset: {val_dataset.shape}\\n\")\n\n    # Creating the Training and Validation dataset for further creation of Dataloader\n    training_set = MyDataset(\n        train_dataset,\n        tokenizer,\n        CFG['source_len'], \n        CFG['target_len']\n    )\n    val_set = MyDataset(\n        val_dataset,\n        tokenizer,\n        CFG['source_len'], \n        CFG['target_len']\n    )\n\n    # Defining the parameters for creation of dataloaders\n    train_params = {\n        \"batch_size\": CFG[\"train_bs\"],\n        \"shuffle\": True,\n        \"num_workers\": 0,\n    }\n\n    val_params = {\n        \"batch_size\": CFG[\"valid_bs\"],\n        \"shuffle\": False,\n        \"num_workers\": 0,\n    }\n\n    # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n    training_loader = DataLoader(training_set, **train_params)\n    val_loader = DataLoader(val_set, **val_params)\n\n    # Defining the optimizer that will be used to tune the weights of the network in the training session.\n    optimizer = torch.optim.Adam(\n        params=model.parameters(), lr=CFG[\"lr\"]\n    )\n\n    # Training loop\n    console.log(f\"[Initiating Fine Tuning]...\\n\")\n\n    for epoch in range(CFG[\"epochs\"]):\n        train(epoch, tokenizer, model, device, training_loader, optimizer)\n\n    console.log(f\"[Saving Model]...\\n\")\n    # Saving the model after training\n    path = os.path.join(output_dir, \"model_files\")\n    model.save_pretrained(path)\n    tokenizer.save_pretrained(path)\n\n    # evaluating test dataset\n    console.log(f\"[Initiating Validation]...\\n\")\n    predictions, actuals = validate(0, tokenizer, model, device, val_loader)\n    final_df = pd.DataFrame({\"Generated Text\": predictions, \"Actual Text\": actuals})\n    final_df.to_csv(os.path.join(output_dir, \"predictions.csv\"))\n\n    console.save_text(os.path.join(output_dir, \"logs.txt\"))\n\n    console.log(f\"[Validation Completed.]\\n\")\n    console.print(\n        f\"\"\"[Model] Model saved @ {os.path.join(output_dir, \"model_files\")}\\n\"\"\"\n    )\n    console.print(\n        f\"\"\"[Validation] Generation on Validation data saved @ {os.path.join(output_dir,'predictions.csv')}\\n\"\"\"\n    )\n    console.print(f\"\"\"[Logs] Logs saved @ {os.path.join(output_dir,'logs.txt')}\\n\"\"\")","metadata":{"execution":{"iopub.status.busy":"2023-10-01T09:30:11.412339Z","iopub.execute_input":"2023-10-01T09:30:11.412858Z","iopub.status.idle":"2023-10-01T09:30:11.428699Z","shell.execute_reply.started":"2023-10-01T09:30:11.412829Z","shell.execute_reply":"2023-10-01T09:30:11.427629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Main","metadata":{}},{"cell_type":"code","source":"if __name__ == '__main__':\n    T5Trainer(df)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T09:30:11.430049Z","iopub.execute_input":"2023-10-01T09:30:11.430620Z"},"trusted":true},"execution_count":null,"outputs":[]}]}